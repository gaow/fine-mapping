{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Fine mapping data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## 1000 Genome data\n",
    "The position of SNPs in 1000 Genome data **start from $0$**, instead of $1$. In order to be consistent with summary statistics, we need to **add $1$** to all 1000 genome SNPs position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Test results 11/07/18\n",
    "\n",
    "|dataset|chunk_653|chunk_654|chunk_655|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|S1|8286510|8286510|8286510|\n",
    "|G1|23749|22424|35634|\n",
    "|S2|169|141|112|\n",
    "|G2|170|141|112|\n",
    "|no strand flip or switch ref/alt|11|10|10|\n",
    "|flipped by strand|73|75|50|\n",
    "|flipped by reference and alternative|13|13|7|\n",
    "|S3|97|98|67|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The nubmer of overlapped SNPs of $S_1$ and $G_1$ is limited, much less than original summary statistics and genotype matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Test results 11/08/18\n",
    "\n",
    "|dataset|chunk_653|chunk_655|\n",
    "|:--:|:--:|:--:|\n",
    "|S1|3080|1944|\n",
    "|G1|23749|35634|\n",
    "|S2|3048|1933|\n",
    "|G2|3050|1933|\n",
    "|no strand flip or switch ref/alt|1397|939|\n",
    "|flipped by strand|0|0|\n",
    "|flipped by reference and alternative|1631|987|\n",
    "|S3|3028|1926|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "Data are prepared based on human LD chunks/blocks.\n",
    "- Summary statistics in a single human LD chunk, including 7 columns\n",
    "    - chromsome\n",
    "    - position\n",
    "    - reference allele\n",
    "    - alternative allele\n",
    "    - ${\\beta}$ (effect size)\n",
    "    - standard error (se)\n",
    "    - z score\n",
    "\n",
    "\n",
    "- LD chunk file. There are 1703 chunks in human genome (exclude X chromosome). Each row represents one block, for example:\n",
    "\n",
    "      1st column is chr; 2nd is chunk start position; 3rd is chunk end position; 4th is chunk number.\n",
    "\n",
    "        chr22\t44995308\t46470495\t1699\n",
    "        chr22\t46470495\t47596318\t1700\n",
    "        chr22\t47596318\t48903703\t1701\n",
    "        chr22\t48903703\t49824534\t1702\n",
    "        chr22\t49824534\t51243298\t1703\n",
    "\n",
    "\n",
    "- Prior information from enrichment analysis. For example:\n",
    "\n",
    "      1st columns format ‚Äúchr:bp:ref:alt‚ÄùÔºå2nd is prior\n",
    "\n",
    "        1:1847856:T:G  1.4413e-04\n",
    "        1:1847979:T:C  7.3716e-05\n",
    "        1:1848109:C:G  1.4413e-04\n",
    "        1:1848160:A:G  1.4413e-04\n",
    "        1:1848734:A:G  7.3716e-05\n",
    "\n",
    "\n",
    "- 1000 Genome genotype file. Available online [here](http://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/). They are listed by chromosome, so it's not necessary to download all. We can only donwload the chromosome where LD chunk located instead.\n",
    "\n",
    "\n",
    "- Population definition: `EUR` (European). Available online [here](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel). There are $503$ European."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Steps\n",
    "\n",
    "- Denote summary statistics matrix in the specific LD chunk $S_1$, and annotation (prior) matrix as $A_1$. The number of row in these two matrices is the number of SNPs in GWAS summary statistics.\n",
    "\n",
    "- Extract EUR (European) population genotype from 1000 Genome genotype file. Rows are SNPs, columns are population genotypes. Transfer genotypes to $0,1,2$ and `nan`. Remove non-variant sites (lines having identical genotypes for everybody). Denote this genotype matrix as $G_1$.\n",
    "\n",
    "        0|0    0\n",
    "        1|0    1\n",
    "        1|1    2\n",
    "        2|0    nan\n",
    "        2|1    nan\n",
    "        2|2    nan\n",
    "\n",
    "\n",
    "    **update** : do not mark `nan`; still use zero instead.\n",
    "\n",
    "- Find overlapped SNPs of $S_1$ and $G_1$, then extract new genotype matrix from $G_1$, denote as $G_2$, and new summary statistics matrix from $S_1$, denote as $S_2$.\n",
    "\n",
    "- Compare $G_2$ and $S_2$'s reference and alternative allele, then generate new summay statistic matrix $S_3$ and new genotype matrix $G_3$. There could be several situations as follows:\n",
    "\n",
    "    - completely identical;\n",
    "    - Not identical, but identical after switching ref and alt in $S_2$: add opposite sign for z score and beta; does not apply to `A/T`, `T/A`, `G/C` or `C/G` for `ref/alt`;\n",
    "    - Not identical, but identical after strand flip ref and alt in $S_2$: keep the sign of z score and beta; does not apply to `A/T`, `T/A`, `G/C` or `C/G` for `ref/alt`;\n",
    "    - Not identical, but identical after strand flip ref and alt then swith their positions: add opposite sign for z score and beta; only apply to `A/G`, `G/A`, `A/C`, `C/A`, `T/C`, `C/T`, `T/G` and `G/T`.\n",
    "    - Not identical, `A/T`, `T/A`, `G/C` or `C/G` for `ref/alt`: consider this situation at last; if flip strand has applied in this LD block, then flip strand and keep the sign of z score and beta; if not, switch ref and alt of $S_2$, add opposite sign for z score and beta.\n",
    "    - Not identical after previous 5 substeps: drop.\n",
    "\n",
    "\n",
    "- Calculate row correlation matrix of $G_3$, denote as $R$. The number of rows and columns of $R$ is the number of SNPs in $G_3$.\n",
    "\n",
    "- Obtain overlapped SNPs for $S_3$ and $A_1$ and use overlapped SNPs to generate new annotation/prior $A_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Outputs\n",
    "- $S_3$: the number of rows of $S_3$ is the same with $A_2$.\n",
    "\n",
    "- $R$: correlation matrix, #SNPs $\\times$ #SNPs of $G_3$.\n",
    "\n",
    "- $A_2$: adjusted annotation/prior information, the number of rows is the same with $S_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/fine_mapping_data"
     ]
    }
   ],
   "source": [
    "%cd /data/fine_mapping_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: WDir=path(\"/data/fine_mapping_data\")\n",
    "# chrom = f'{WDir:b}'.split(\"_\")[0][3:]\n",
    "# paths to input files\n",
    "# parameter: kgenome=path(f\"{WDir}/1000Genome/ALL.chr{chrom}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\")\n",
    "parameter: kgenome_cwd=path(f\"{WDir}/1000Genome\")\n",
    "parameter: IDrace=\"EUR\"\n",
    "parameter: IDfile=path(f\"{WDir}/1000Genome/integrated_call_samples_v3.20130502.ALL.panel.txt\")\n",
    "parameter: LDchunk = path(f\"{WDir}/chunk.dat\")\n",
    "parameter: summaryfile = path(f\"{WDir}/GWAS/Summary_statistics.gz\")\n",
    "parameter: anno = \"atac-seq_asca\"\n",
    "annofile = path(f\"{WDir}/annotation/Annotation_{anno}.gz\")\n",
    "# Use --no-strand-flip to set it to false\n",
    "parameter: strand_flip = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Obtain EUR genotype data\n",
    "According to integrated_call_samples_v3.20130502.ALL.panel.txt EUR sample ID, export genotype information in ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get information about a specified race\n",
    "[prepare_reference_vcf]\n",
    "depends: executable(\"bcftools\"), executable(\"tabix\")\n",
    "input: f\"{kgenome_cwd}/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\", # group_by = 1, concurrent = True\n",
    "output: f\"{_input:bnn}.{IDrace}.vcf.gz\"\n",
    "bash: expand = True\n",
    "    grep -w \"{IDrace}\" {IDfile} | cut -f 1 > {_output:nn}_extracted.txt\n",
    "    bcftools view -S {_output:nn}_extracted.txt {_input} -Oz > {_output}\n",
    "    tabix -p vcf {_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# !sos run /home/min/GIT/atac-gwas/fineMapping/20181101_FineMapping_Data_preparation_workflow.ipynb prepare_reference_vcf -s build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Rename chrX\n",
    "```\n",
    "cd /data/fine_mapping_data/1000Genome/\n",
    "mv ALL.chrX.phase3_shapeit2_mvncall_integrated_v1b.20130502.genotypes.EUR_extracted.txt ALL.chrX.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.EUR_extracted.txt\n",
    "mv ALL.chrX.phase3_shapeit2_mvncall_integrated_v1b.20130502.genotypes.EUR.vcf.gz ALL.chrX.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.EUR.vcf.gz\n",
    "mv ALL.chrX.phase3_shapeit2_mvncall_integrated_v1b.20130502.genotypes.EUR.vcf.gz.tbi ALL.chrX.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.EUR.vcf.gz.tbi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Obtain genotype matrix `ùê∫1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_1]\n",
    "import pandas as pd\n",
    "## FIXME: chrX snps, LD chunk does not include chromosome X\n",
    "chunks = [x.tolist() for idx, x in pd.read_table(f'{LDchunk}',header=None,sep='\\s+').iterrows() if not x[0].startswith('#')]\n",
    "print (chunks)\n",
    "input: for_each = 'chunks', concurrent = True\n",
    "output: f\"{WDir:d}/fine_mapping/{_chunks[0]}_{_chunks[-1]}/{summaryfile:bn}/chunk_{_chunks[-1]}.pkl\"\n",
    "kgenome = path(f'{kgenome_cwd}/ALL.chr{_chunks[0][3:]}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.{IDrace}.vcf.gz')\n",
    "python3: expand=\"${ }\"\n",
    "    from cyvcf2 import VCF\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    chromosome=\"${_chunks[0]}\".replace(\"chr\",\"\")\n",
    "    tit_tmp = pd.read_table(\"${kgenome:nn}_extracted.txt\", header=None)\n",
    "    tit = tit_tmp.T\n",
    "    se = pd.Series([\"index\",\"ID\",\"ref\",\"alt\"])\n",
    "    ss = se.append(tit.iloc[0,:].map(str))\n",
    "    # chunk region\n",
    "    queryid=str(chromosome)+\":\"+str(\"${_chunks[1]}\")+\"-\"+str(\"${_chunks[2]}\")\n",
    "    # scan VCF chunk\n",
    "    vcf = VCF(${kgenome:r}, gts012=False)\n",
    "    res = []\n",
    "    ## Attention: 1000 Genome position start from 0, not 1! So need to +1 for variant.start\n",
    "    for variant in vcf(queryid):\n",
    "        for i in range(len(variant.ALT)):\n",
    "            line = [f'{variant.CHROM}:{variant.start+1}:{variant.REF}:{variant.ALT[i]}', \n",
    "                    f'{variant.CHROM}:{variant.start+1}', variant.REF, variant.ALT[i]] + \\\n",
    "                    [x[:-1].count(i+1) for x in variant.genotypes] # do not use else np.nan because it will mess up the cor matrix (will not be P.D.)\n",
    "            if len(set(line[4:])) == 1:\n",
    "                # remove non-variant site in 1000 data\n",
    "                continue\n",
    "            res.append(line)\n",
    "    gt = pd.DataFrame(res, columns = ss)\n",
    "    gt.set_index('index',inplace=True)\n",
    "    gt.to_pickle(${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Obtain genotype matrix ùê∫2, summary statistics ùëÜ2, compare ref/alt in ùëÜ2 and ùê∫2 => ùëÜ3 and ùê∫3, calculate ùëÖ=row_corr(ùê∫3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_2]\n",
    "input: group_by = 1, concurrent = True\n",
    "output: f\"{_input:n}.matched_ss.txt\", f\"{_input:n}.LD.txt\"\n",
    "python3: expand=\"${ }\", stdout=f'{_input:n}.flip_summary'\n",
    "    import pandas as pd, numpy as np\n",
    "    import datetime, sys\n",
    "    print(datetime.datetime.now(),\"\\n\")\n",
    "    n_chunk = int(\"${_input:bn}\".split(\"_\")[-1])\n",
    "    chunks = pd.read_table(f'${LDchunk}',header=None,sep='\\s+')\n",
    "    chunk_info = chunks[chunks[3] == n_chunk].iloc[0, 0:3].tolist()\n",
    "    S1 = pd.read_table(\"${summaryfile}\",compression='gzip',header = 0)\n",
    "    S1 = S1[(S1[\"chr\"] == chunk_info[0]) & (S1[\"bp\"] >= int(chunk_info[1])) & (S1[\"bp\"] < int(chunk_info[2]))]\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\\n\" % (\"S1\",len(S1)))\n",
    "    if S1.empty:\n",
    "        open(\"${_output[0]}\", 'w').close()\n",
    "        open(\"${_output[1]}\", 'w').close()\n",
    "        sys.exit(0)\n",
    "    S1[\"chr\"] = S1[\"chr\"].str.split(\"chr\",expand=True)[1]\n",
    "    S1[\"ID\"] = S1[\"chr\"].map(str)+\":\"+S1[\"bp\"].map(str)\n",
    "    G1 = pd.read_pickle(\"${_input}\")\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\\n\" % (\"G1\",len(G1)))\n",
    "    # S2\n",
    "    S2 = S1[S1[\"ID\"].isin(G1[\"ID\"])]\n",
    "    #S2[\"ID\"] = S2[\"ID\"].fillna\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\\n\" % (\"S2\",len(S2)))\n",
    "    # ùê∫2\n",
    "    G2 = G1[G1[\"ID\"].isin(S1[\"ID\"])]\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\\n\" % (\"G2\",len(G2)))\n",
    "    # ùëÜ2 and ùê∫2 : reference and alternative\n",
    "    # at ta cg gc\n",
    "    def gt(s1,s2,s3,s4):\n",
    "        if (s1+s2 == \"AT\" and s3+s4 == \"TA\") or (s1+s2 == \"GC\" and s3+s4 == \"CG\" ) or (s1+s2 == \"TA\" and s3+s4 == \"AT\") or (s1+s2 == \"CG\" and s3+s4 == \"GC\"):\n",
    "            return ${0 if strand_flip else 1}\n",
    "        else:\n",
    "            return 1\n",
    "    # flip\n",
    "    def atcg(inp):\n",
    "        if inp ==\"A\":\n",
    "            return \"T\"\n",
    "        elif inp == \"T\":\n",
    "            return \"A\"\n",
    "        elif inp == \"G\":\n",
    "            return \"C\"\n",
    "        elif inp == \"C\":\n",
    "            return \"G\"\n",
    "    # S3 \n",
    "    LDFLIP=\"N\"\n",
    "    FLIPD=[]\n",
    "    # equal\n",
    "    EQUAL = 0\n",
    "    # flipped by strand \n",
    "    FLIPNUM = 0\n",
    "    # flipped by reference and alternative\n",
    "    REFALTNUM = 0 \n",
    "    \n",
    "    for i in range(len(S2)):\n",
    "        line = list(S2.iloc[i,:2])\n",
    "        if set([S2.iloc[i,2],S2.iloc[i,3]]) != set([G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"][0],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"][0]]):\n",
    "            print (S2.iloc[i,1],S2.iloc[i,2],S2.iloc[i,3],S2.iloc[i,-1], G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"][0],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"][0])\n",
    "        # not at/ta/gc/cg\n",
    "        if gt(S2.iloc[i,2],S2.iloc[i,3],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"][0],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"][0])==1:\n",
    "            if G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"][0] == S2.iloc[i,2] and G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"][0] == S2.iloc[i,3]:\n",
    "                line=S2.iloc[i,:].tolist()\n",
    "                EQUAL+=1\n",
    "                FLIPD.append(line)\n",
    "            elif G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"][0] == S2.iloc[i,2] and G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"][0] == S2.iloc[i,3]:\n",
    "                REFALTNUM+=1\n",
    "                line.extend([S2.iloc[i,3],S2.iloc[i,2],0-S2.iloc[i,4],0-S2.iloc[i,5],S2.iloc[i,6]])\n",
    "                FLIPD.append(line)\n",
    "            elif atcg(S2.iloc[i,2]) == G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"][0] and atcg(S2.iloc[i,3]) == G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"][0]:\n",
    "                LDFLIP=\"Y\"\n",
    "                FLIPNUM+=1\n",
    "                line.extend([atcg(S2.iloc[i,2]),atcg(S2.iloc[i,3]),S2.iloc[i,4],S2.iloc[i,5],S2.iloc[i,6]])\n",
    "                FLIPD.append(line)\n",
    "            elif atcg(S2.iloc[i,2]) == G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"][0] and atcg(S2.iloc[i,3]) == G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"][0]:\n",
    "                LDFLIP=\"Y\"\n",
    "                FLIPNUM+=1\n",
    "                tmp = S2.iloc[i,2]\n",
    "                line.extend([atcg(S2.iloc[i,3]),atcg(tmp),0-S2.iloc[i,4],0-S2.iloc[i,5],S2.iloc[i,6]])\n",
    "                FLIPD.append(line)\n",
    "        # at/ta/cg/gc AND flip\n",
    "        elif gt(S2.iloc[i,2],S2.iloc[i,3],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"][0],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"][0])==0 and LDFLIP==\"Y\":\n",
    "            FLIPNUM+=1\n",
    "            line.extend([atcg(S2.iloc[i,2]),atcg(S2.iloc[i,3]),S2.iloc[i,4],S2.iloc[i,5],S2.iloc[i,6]])\n",
    "            FLIPD.append(line)\n",
    "        # at/ta/cg/gc AND not flip\n",
    "        elif gt(S2.iloc[i,2],S2.iloc[i,3],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"][0],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"][0])==0 and LDFLIP==\"N\":\n",
    "            REFALTNUM+=1\n",
    "            line.extend([atcg(S2.iloc[i,3]),atcg(S2.iloc[i,2]),0-S2.iloc[i,4],0-S2.iloc[i,5],S2.iloc[i,6]])\n",
    "            FLIPD.append(line)\n",
    "            \n",
    "    print(\"equal:%s\\tflipped by strand:%d\\tflipped by reference and alternative:%d\\n\" % (EQUAL,FLIPNUM,REFALTNUM))\n",
    "    S3 = pd.DataFrame(FLIPD)\n",
    "    if S3.shape[1] == 7:\n",
    "        S3.columns=[\"chr\",\"bp\",\"ref\",\"alt\",\"z\",\"beta\",\"se\"]\n",
    "    else:\n",
    "        S3.columns=[\"chr\",\"bp\",\"ref\",\"alt\",\"z\",\"beta\",\"se\",\"ID\"]\n",
    "    S3[\"ID\"] = S3[\"chr\"].map(str)+\":\"+S3[\"bp\"].map(str)\n",
    "    # ùê∫3\n",
    "    G3 = G2[G2[\"ID\"].isin(S3[\"ID\"])]\n",
    "    G3 = G3.drop_duplicates(subset = [\"ID\"])\n",
    "    # S3 OUT\n",
    "    S3 = S3.drop(columns=['ID'])\n",
    "    S3.sort_values(by = [\"chr\", \"bp\"]).to_csv(\"${_output[0]}\",sep=\"\\t\",index=False)\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\\n\" % (\"S3\",len(S3)))\n",
    "    # ùëÖ=row_corr(ùê∫3) OUT \n",
    "    G3 = G3.drop(columns=['ID',\"ref\",\"alt\"]).values\n",
    "    np.savetxt(\"${_output[1]}\", np.corrcoef(G3), fmt = '%.5f')\n",
    "#_input.zap()\n",
    "\n",
    "bash: expand=\"${ }\"\n",
    "    cat ${_output[0]} | awk '{print $1\":\"$2\"\\t\"$6\"\\t\"$7}' | tail -n +2 > ${_output[0]:nn}.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "cd /data/05-Nov-2018/Summary_statistics\n",
    "cat chunk_5.matched_ss.txt | awk '{print $1\":\"$2\"\\t\"$6\"\\t\"$7}' | tail -n +2 > chunk_5.dat\n",
    "cat chunk_5.matched_ss.txt | awk '{print $1\":\"$2\"\\t\"$5}' | tail -n +2 > chunk_5.dat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Obtain annotation `ùê¥2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_3]\n",
    "input: group_by = 2, concurrent = True\n",
    "print(_input)\n",
    "output: f\"{_input[0]:nn}.{annofile:bn}.txt\"\n",
    "stop_if(os.stat(_input[0]).st_size == 0)\n",
    "python3: expand=\"${ }\"\n",
    "    import pandas as pd\n",
    "    A1 = pd.read_table(\"${annofile}\", compression='gzip', sep=\"\\s+\", header=None)\n",
    "    A1.columns=[\"ID\",\"VALUE\"]\n",
    "    A1[\"ID_2\"] = A1[\"ID\"].apply(lambda x: ':'.join(map(str, x.split(\":\")[0:2])))\n",
    "    S3 = pd.read_table(\"${_input[0]}\",sep=\"\\t\")\n",
    "    S3[\"ID\"]=S3.iloc[:,0].map(str)+\":\"+S3.iloc[:,1].map(str)#+\":\"+S3.iloc[:,2]+\":\"+S3.iloc[:,3]\n",
    "    A2 = A1[A1[\"ID_2\"].isin(S3[\"ID\"])][[\"ID_2\",\"VALUE\"]]\n",
    "    A2 = A2.drop_duplicates(subset = [\"ID_2\"])\n",
    "    A2.to_csv(\"${_output}\",sep=\"\\t\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mdefault_1\u001b[0m: \n",
      "[['chr5', 140023664, 140222664, 108]]\n",
      "INFO: Step \u001b[32mdefault_1\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: output:   \u001b[32m/data/fine_mapping/chr5_108/Summary_statistics/chunk_108.pkl\u001b[0m\n",
      "INFO: Running \u001b[32mdefault_2\u001b[0m: \n",
      "INFO: Step \u001b[32mdefault_2\u001b[0m (index=0) is \u001b[32mignored\u001b[0m with signature constructed\n",
      "INFO: output:   \u001b[32m/data/fine_mapping/chr5_108/Summary_statistics/chunk_108.matched_ss.txt /data/fine_mapping/chr5_108/Summary_statistics/chunk_108.LD.txt\u001b[0m\n",
      "INFO: Running \u001b[32mdefault_3\u001b[0m: \n",
      "INFO: output:   \u001b[32m/data/fine_mapping/chr5_108/Summary_statistics/chunk_108.Annotation_atac-seq_asca.txt\u001b[0m\n",
      "INFO: Workflow default (ID=6aea0b9f1f7f3627) is executed successfully with 1 completed step and 2 ignored steps.\n"
     ]
    }
   ],
   "source": [
    "!sos run /home/min/GIT/atac-gwas/fineMapping/20181101_FineMapping_Data_preparation_workflow.ipynb --no-strand_flip --anno atac-seq_asca -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#!sos run /home/min/GIT/atac-gwas/fineMapping/20181101_FineMapping_Data_preparation_workflow.ipynb --no-strand_flip --anno atac-seq -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#!sos run /home/min/GIT/atac-gwas/fineMapping/20181101_FineMapping_Data_preparation_workflow.ipynb --no-strand_flip --anno general -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#!sos run /home/min/GIT/atac-gwas/fineMapping/20181101_FineMapping_Data_preparation_workflow.ipynb --no-strand_flip --anno all -s build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Results of LD chunk 655"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Flip summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">%preview /data/chr12_4/Summary_statistics/chunk_4.flip_summary</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> /data/chr12_4/Summary_statistics/chunk_4.flip_summary (451 B):</div>"
      ],
      "text/plain": [
       "\n",
       "> /data/chr12_4/Summary_statistics/chunk_4.flip_summary (451 B):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">28 lines (20 displayed, see --limit)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-19 22:48:44.925929 \n",
      "\n",
      "SAMPLEÔºöS1\tLENGTH:596\n",
      "\n",
      "SAMPLEÔºöG1\tLENGTH:1792\n",
      "\n",
      "SAMPLEÔºöS2\tLENGTH:588\n",
      "\n",
      "SAMPLEÔºöG2\tLENGTH:588\n",
      "\n",
      "equal:283\tflipped by strand:0\tflipped by reference and alternative:303\n",
      "\n",
      "SAMPLEÔºöS3\tLENGTH:586\n",
      "\n",
      "2018-11-19 23:39:44.591608 \n",
      "\n",
      "SAMPLEÔºöS1\tLENGTH:4933\n",
      "\n",
      "SAMPLEÔºöG1\tLENGTH:13745\n"
     ]
    }
   ],
   "source": [
    "%preview /data/chr12_4/Summary_statistics/chunk_4.flip_summary -n --limit 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The number of SNPs in \n",
    "\n",
    "- S1: 8286510\n",
    "- G1: 35634\n",
    "- S2: 112\n",
    "- G2: 112\n",
    "- S3: 67\n",
    "\n",
    "The number of identical SNPs in S2 and G2 is 10; strand flip 50 SNPs; switch ref/alt 7 SNPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Summary statistics matrix S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">%preview Summary_statistics/chunk_1696.matched_ss.txt</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> Summary_statistics/chunk_1696.matched_ss.txt (205.3 KiB):</div>"
      ],
      "text/plain": [
       "\n",
       "> Summary_statistics/chunk_1696.matched_ss.txt (205.3 KiB):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">4425 lines (5 displayed, see --limit)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr\tbp\tref\talt\tz\tbeta\tse\n",
      "22\t40546153\tG\tA\t2.181181\t0.023903\t0.011\n",
      "22\t40546525\tA\tG\t-2.058627\t-0.076201\t0.037000000000000005\n",
      "22\t40546633\tC\tT\t2.197286\t0.023699\t0.0108\n",
      "22\t40547688\tA\tG\t0.367294\t0.006797\t0.0184"
     ]
    }
   ],
   "source": [
    "%preview Summary_statistics/chunk_1696.matched_ss.txt -n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "### Annotation/prior matrix A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">%preview Summary_statistics/chunk_1696.annotation.txt</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> Summary_statistics/chunk_1696.annotation.txt (99.2 KiB):</div>"
      ],
      "text/plain": [
       "\n",
       "> Summary_statistics/chunk_1696.annotation.txt (99.2 KiB):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">4426 lines (5 displayed, see --limit)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:40546153\t7.3409e-05\n",
      "22:40546525\t7.3409e-05\n",
      "22:40546633\t7.3409e-05\n",
      "22:40547688\t7.3409e-05\n",
      "22:40548084\t7.3409e-05"
     ]
    }
   ],
   "source": [
    "%preview Summary_statistics/chunk_1696.annotation.txt -n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### ùëÖ: LD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# %preview Summary_statistics/chunk_1696.LD.txt -n -l 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.17.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
