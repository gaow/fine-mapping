{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Fine-mapping result post processing\n",
    "\n",
    "This pipeline consolidates results from various fine-mapping tools to uniform format, add rsID as necessary, and perform a simple \"liftover\" via rsID (not the formal UCSC `liftOver`) to generate output in HG37 and HG38 builds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This pipeline was devised by Gao Wang and implemented by Gao Wang and Kushal Dey at Harvard University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input data\n",
    "\n",
    "Input are results of fine-mapping pipeline `summary_statistics_finemapping.ipynb` in R's `RDS` format for SuSiE and CAVIAR, and `pkl` format for DAP.\n",
    "\n",
    "## Output data\n",
    "\n",
    "columns are:\n",
    "\n",
    "```\n",
    "chr pos ref alt snp_id locus_id PIP CS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "where `snp_id` will be rsID if some annotation files on rsID are provided. Otherwise it will take the format of `chr:pos:ref:alt`.\n",
    "\n",
    "## Additional data processing\n",
    "\n",
    "Here we also provide additional routines to process the data,\n",
    "\n",
    "1. Swap the `snp_id` column using external annotations, for example by rsID.\n",
    "2. \"liftover\" to other builds -- we only support it via rsID matching\n",
    "\n",
    "Notice that only the first 5 columns are necessary for these additional operations. The columns after the fifth can be arbitary and will be kept during the process.\n",
    "\n",
    "- To trigger optional step 1, parameter `--id-map-prefix` and `--id-map-suffix` have to be valid.\n",
    "- To trigger optional step 2, parameter `--coordinate-map-prefix` and `--coordinate-map-suffix` have to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gaow/Documents/GIT/github/fine-mapping"
     ]
    }
   ],
   "source": [
    "%cd ~/GIT/github/fine-mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## The workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run workflow/finemapping_results_wrangler.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  default\n",
      "\n",
      "Global Workflow Options:\n",
      "  --ss-data-prefix . (as path)\n",
      "                        summary statistics file prefix which is the path to all\n",
      "                        output files\n",
      "  --pattern 'uniform.SuSiE_B.L_5.prior_0p005.res_var_false'\n",
      "                        identifier for fine-mapping results to be extracted\n",
      "\n",
      "Sections\n",
      "  default_1:            Consolidate fine-mapping results\n",
      "    Workflow Options:\n",
      "      --pip-thresh 0.05 (as float)\n",
      "                        Keep PIP above these thresholds\n",
      "      --round-off 6 (as int)\n",
      "                        Round PIP to given digits\n",
      "  default_2:            Update variant ID based on chrom and pos (optional)\n",
      "    Workflow Options:\n",
      "      --id-map-prefix . (as path)\n",
      "                        Path containing files for variant ID update rule Each\n",
      "                        file is a separate chromsome\n",
      "      --id-map-suffix '.bim'\n",
      "      --columns 2 4 (as list)\n",
      "                        columns first element for variant ID, 2nd element for\n",
      "                        genomic position is [2,4] for BIM files\n",
      "      --chroms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 (as list)\n",
      "                        chromosome identifiers\n",
      "  default_3:            Update genomic coordinates based on variant ID\n",
      "                        (optional)\n",
      "    Workflow Options:\n",
      "      --coordinate-map-prefix . (as path)\n",
      "                        Path containing files for coordinate update rule Each\n",
      "                        file is a separate chromsome\n",
      "      --coordinate-map-suffix '.bim'\n",
      "      --coordinate-version-id hgX\n",
      "                        coordinate identifier\n",
      "      --columns 2 4 5 6 (as list)\n",
      "                        columns first element for variant ID, 2nd element for\n",
      "                        genomic position 3rd for reference allele and 4th for\n",
      "                        alternative allele is [2,4,5,6] for BIM files\n",
      "      --chroms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 (as list)\n",
      "                        chromosome identifiers\n"
     ]
    }
   ],
   "source": [
    "sos run workflow/finemapping_results_wrangler.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# summary statistics file prefix\n",
    "# which is the path to all output files\n",
    "parameter: ss_data_prefix = path()\n",
    "# identifier for fine-mapping results to be extracted\n",
    "parameter: pattern = \"uniform.SuSiE_B.L_5.prior_0p005.res_var_false\"\n",
    "\n",
    "if 'SuSiE' in pattern:\n",
    "    source = 'susie'\n",
    "elif 'CAVIAR' in pattern:\n",
    "    source = 'CAVIAR'\n",
    "elif 'DAP' in pattern:\n",
    "    source = 'DAP'\n",
    "else:\n",
    "    raise ValueError(\"Invalid --pattern specification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Fine-mapping results consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Consolidate fine-mapping results\n",
    "[default_1 (extract results to single file)]\n",
    "depends: R_library('dscrutils') # can be installed via `devtools::install_github(\"stephenslab/dsc\",subdir = \"dscrutils\", force = TRUE)`\n",
    "# Keep PIP above these thresholds\n",
    "parameter: pip_thresh = 0.05\n",
    "# Round PIP to given digits\n",
    "parameter: round_off = 6\n",
    "input: [glob.glob(f'{ss_data_prefix:a}/*/*.{pattern}.{ext}') for ext in ['rds', 'pkl']]\n",
    "output: f\"{ss_data_prefix:a}.{pattern}.gz\"\n",
    "fail_if(len(_input) == 0, msg = f'Cannot find valid input files by pattern {ss_data_prefix:a}/*/*.{pattern}.[rds,pkl]')\n",
    "R: expand = \"${ }\", workdir = ss_data_prefix, stdout = f'{_output:n}.log'\n",
    "    # Here we define get_*_output functions for different output format\n",
    "    get_susie_output = function(unit, rds_file) {\n",
    "        cs_id = rep(0, length(rds_file$var_names))\n",
    "        num_cs = length(rds_file$sets$cs)\n",
    "        for(id in 1:num_cs){\n",
    "            cs_id[rds_file$sets$cs[[id]]] = id\n",
    "        }\n",
    "        cbind.data.frame(rep(unit, length(rds_file$var_names)), \n",
    "                              rds_file$var_names, \n",
    "                              rds_file$pip, cs_id)\n",
    "    }\n",
    "    # Data extraction script\n",
    "    library(data.table)\n",
    "    files = c(${paths([x.relative_to(ss_data_prefix) for x in _input]):r,})\n",
    "    processed_dat = c()\n",
    "    for (f in files) {\n",
    "      rds_file = dscrutils::read_dsc(f)\n",
    "      unit = dirname(f)\n",
    "      processed_dat_temp = get_${source}_output(unit, rds_file)\n",
    "      colnames(processed_dat_temp) = c(\"locus_id\", \"variant_id\", \"pip\", \"cs\")\n",
    "      processed_dat = rbind(processed_dat, processed_dat_temp[which(processed_dat_temp[,3] >= ${pip_thresh} | processed_dat_temp[,4] > 0), ])\n",
    "      cat(\"We are at unit\", unit, \"\\n\")\n",
    "    }\n",
    "\n",
    "    extract_chr = sapply(processed_dat[,2], function(x) return(as.numeric(strsplit(as.character(x), \":\")[[1]][1])))\n",
    "    extract_pos = sapply(processed_dat[,2], function(x) return(as.numeric(strsplit(as.character(x), \":\")[[1]][2])))\n",
    "    extract_ref = sapply(processed_dat[,2], function(x) return(as.character(strsplit(as.character(x), \":\")[[1]][3])))\n",
    "    extract_alt = sapply(processed_dat[,2], function(x) return(as.character(strsplit(as.character(x), \":\")[[1]][4])))\n",
    "    variant_id = processed_dat[,2]\n",
    "    locus_id = processed_dat[,1]\n",
    "    pip = round(processed_dat[,3], ${round_off})\n",
    "    cs = processed_dat[,4]\n",
    "    df = data.frame(\"chr\" = extract_chr, \"pos\" = extract_pos, \"ref\" = extract_ref, \"alt\" = extract_alt, \n",
    "                    \"variant_id\" = variant_id, \"locus_id\" = locus_id, \"pip\" = pip, \"cs\" = cs)\n",
    "    df_sorted = df [order(df[,1], df[,2]),]\n",
    "    write.table(df_sorted, gzfile(${_output:r}), sep = \"\\t\", quote=FALSE, row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Update variant ID via genomic coordinantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Update variant ID based on chrom and pos (optional)\n",
    "[default_2 (update variant ID from per chrom files)]\n",
    "depends: R_library('rlang>=0.3.0'), R_library('dplyr'), R_library('data.table')\n",
    "# Path containing files for variant ID update rule\n",
    "# Each file is a separate chromsome\n",
    "parameter: id_map_prefix = path()\n",
    "parameter: id_map_suffix = '.bim'\n",
    "# columns first element for variant ID, 2nd element for genomic position\n",
    "# is [2,4] for BIM files\n",
    "parameter: columns = [2,4]\n",
    "# chromosome identifiers\n",
    "parameter: chroms = [x+1 for x in range(22)]\n",
    "stop_if(len(glob.glob(f\"{id_map_prefix:a}.*.{id_map_suffix}\")) == 0, msg = 'Variant ID are not updated because no valid file is found using --id-map-prefix and --id-map-suffix')\n",
    "output: f'{_input:n}.var_id_updated.gz'\n",
    "R: expand = \"${ }\", stdout = f'{_output:n}.log'\n",
    "    suppressMessages(library(dplyr))\n",
    "    suppressMessages(library(data.table))\n",
    "    out =  data.frame(fread(\"zcat ${_input}\"))\n",
    "    out %>% mutate_if(is.factor, as.character) -> out\n",
    "    chroms = c(${paths(chroms):r,})\n",
    "    for(numchr in chroms){\n",
    "        which_chr = which(out$chr == numchr)\n",
    "        out_sub = out[which_chr, ]\n",
    "        dbfile = data.frame(fread(paste0(\"${id_map_prefix}.\", numchr, \".${id_map_suffix}\")))\n",
    "        out_sub_new = out_sub\n",
    "        idx1 = match(out_sub$pos, dbfile$V${columns[1]})\n",
    "        idx2 = idx1[which(!is.na(idx1))]\n",
    "        idx3 = 1:length(out_sub$pos)\n",
    "        idx4 = idx3[which(!is.na(idx1))]\n",
    "        out_sub_new[idx4, \"variant_id\"] = dbfile[idx2,\"V${columns[0]}\"]\n",
    "        out[which_chr, ] = out_sub_new\n",
    "        cat(\"Variant IDs updated for chromosome\", numchr, \"\\n\")\n",
    "    }\n",
    "    write.table(out, gzfile(${_output:r}), sep = \"\\t\", quote=FALSE, row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Update genomic coordinates via variant ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Update genomic coordinates based on variant ID (optional)\n",
    "[default_3 (update genomic coordinates from per chrom files)]\n",
    "depends: R_library('rlang>=0.3.0'), R_library('dplyr'), R_library('data.table')\n",
    "# Path containing files for coordinate update rule\n",
    "# Each file is a separate chromsome\n",
    "parameter: coordinate_map_prefix = path()\n",
    "parameter: coordinate_map_suffix = '.bim'\n",
    "# coordinate identifier\n",
    "parameter: coordinate_version_id = 'hgX'\n",
    "# columns first element for variant ID, 2nd element for genomic position\n",
    "# 3rd for reference allele and 4th for alternative allele\n",
    "# is [2,4,5,6] for BIM files\n",
    "parameter: columns = [2,4,5,6]\n",
    "# chromosome identifiers\n",
    "parameter: chroms = [x+1 for x in range(22)]\n",
    "stop_if(len(glob.glob(f\"{coordinate_map_prefix:a}.*.{coordinate_map_suffix}\")) == 0, msg = 'Genomic coordinates are not updated because no valid file is found using --coordinate-map-prefix and --coordinate-map-suffix')\n",
    "output: f'{_input:n}.{coordinate_version_id}.gz'.replace('.var_id_updated.', '.')\n",
    "R: expand = \"${ }\", stdout = f'{_output:n}.log'\n",
    "    suppressMessages(library(dplyr))\n",
    "    suppressMessages(library(data.table))\n",
    "    out =  data.frame(fread(\"zcat ${_input}\"))\n",
    "    out %>% mutate_if(is.factor, as.character) -> out2\n",
    "    chroms = c(${paths(chroms):r,})\n",
    "    for(numchr in chroms){\n",
    "        dbfile = data.frame(fread(paste0(\"${coordinate_map_prefix}.\", numchr, \".${coordinate_map_suffix}\")))\n",
    "        out2_sub = out2[which(out2$chr == numchr), ]\n",
    "        idx1 = match(out2_sub$variant_id, dbfile$V${columns[0]})\n",
    "        idx2 = idx1[which(!is.na(idx1))]\n",
    "        idx3 = 1:length(out2_sub$variant_id)\n",
    "        idx4 = idx3[which(!is.na(idx1))]\n",
    "        out2_sub_new = out2_sub\n",
    "        out2_sub_new[idx4, c(\"pos\", \"ref\", \"alt\")] = dbfile[idx2, c(\"V${columns[1]}\", \"V${columns[2]}\", \"V${columns[3]}\")]\n",
    "        out2[which(out2$chr == numchr), ] = out2_sub_new\n",
    "        cat(\"Genomic coordinate updated for chromosome\", numchr, \"\\n\")\n",
    "    }\n",
    "    write.table(out2, gzfile(${_output:r}), sep = \"\\t\", quote=FALSE, row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run workflow/finemapping_results_wrangler.ipynb \\\n",
    "    --ss-data-prefix ~/tmp/01-Jan-2019 \\\n",
    "    --pattern uniform.SuSiE_B.L_5.prior_0p005.res_var_false \\\n",
    "    --id-map-prefix /project2/mstephens/SuSIE_gtex_CPP/rsID_map/Hg38/1000G.EUR.hg38 \\\n",
    "    --id-map-suffix bim \\\n",
    "    --coordinate-map-prefix /project2/mstephens/SuSIE_gtex_CPP/rsID_map/Hg37/1000G.EUR.QC \\\n",
    "    --coordinate-map-suffix bim \\\n",
    "    --coordinate-version-id hg37"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.17.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
