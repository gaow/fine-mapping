{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Summary statistics data mungling in preparation for fine-mapping pipelines\n",
    "\n",
    "This pipeline extracts loci of interest from association analysis summary statistics data, intersecting it with genotype data to compute correlation matrix, and output the data-set per loci with summary statistics and genotype correlations matched. Additionally it extracts prior inclusion probability (annotation scores) for each variant in the data-set, if the information is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This pipeline was devised by Gao Wang and implemented by Min Qiao at The University of Chicago. It can be downloaded [from here](https://github.com/gaow/fine-mapping/tree/master/workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "- Summary statistics file, `gzip` compressed, containing information of 6 or 7 columns of core information\n",
    "    - chromsome\n",
    "    - position\n",
    "    - reference allele\n",
    "    - alternative allele\n",
    "    - [${\\beta}$ (effect size) and standard error (se)] or $z$ score\n",
    "    \n",
    "  The input file does not have to follow this ordering. It can be specified by `--columns` parameter later. Additionally it is possible to input a column `loci ID` using `--loci-column`. It is relevant for QTL analysis where the same position can belong to different loci.    \n",
    "\n",
    "- Loci file, similar in flavor to `bed` files.\n",
    "\n",
    "      1st column is chr; 2nd is chunk start position; 3rd is chunk end position; 4th is loci identifier.\n",
    "\n",
    "        chr22\t44995308\t46470495\t1699\n",
    "        chr22\t46470495\t47596318\t1700\n",
    "        chr22\t47596318\t48903703\t1701\n",
    "        chr22\t48903703\t49824534\t1702\n",
    "        chr22\t49824534\t51243298\t1703\n",
    "        \n",
    "  If the last column is not available the first 3 columns will be concatenated to become the loci identifier. \n",
    "\n",
    "  Note that default data-base for loci can be, naturally, LD blocks. For example European genomes are typically divided into 1703 LD chunks (exclude X chromosome).\n",
    "\n",
    "\n",
    "- Prior inclusion probability, for example:\n",
    "\n",
    "      1st columns format “chr:bp:ref:alt”，2nd is prior\n",
    "\n",
    "        1:1847856:T:G  1.4413e-04\n",
    "        1:1847979:T:C  7.3716e-05\n",
    "        1:1848109:C:G  1.4413e-04\n",
    "        1:1848160:A:G  1.4413e-04\n",
    "        1:1848734:A:G  7.3716e-05\n",
    "        \n",
    "  This is the default format from the enrichment analysis tool called `torus`.\n",
    "  \n",
    "- Genotype data reference panel (or panels if by chromosome), in VCF format. Ideally this is the genotype data used to generate the summary statistics; but external reference panel can also be used. \n",
    "A popular choice is [1000 Genomes (data download)](http://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3) for genotypes of \n",
    "[different population (data download)](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel).\n",
    "    - There are 503 Europeans (`EUR`) in 1000 Genomes data.\n",
    "  \n",
    "  For genotype in multiple VCF files the input have to be a 'manifest' file under the same directory with the VCF files and reads like:\n",
    "  ```\n",
    "  1 ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.EUR.vcf.gz\n",
    "  2 ALL.chr2.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.EUR.vcf.gz\n",
    "  ...\n",
    "  ```\n",
    "  where each line is white-space separated with the first the chromosome number and second the file name.\n",
    "  \n",
    "  **For UChicago midway users**: reference genotype data for 1000 Genomes EUR samples are extracted using workflow `prepare_1KG_reference` below. The output can be found under `/project/mstephens/SuSIE_gtex_CPP/1KG_EUR`. The manifest file `1KG_EUR.manifest` was prepared by:\n",
    "  ```\n",
    "  for i in {1..22} X; do echo $i ALL.chr$i.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.EUR.vcf.gz; done > 1KG_EUR.manifest\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The end of this workflow notebook has some example commands running on some toy data. These toy data (500MB; did not bother to make it smaller due to my laziness) [can be downloaded from here](http://shiny.stephenslab.uchicago.edu/gaow/finemapping_summary_stats_example.tar.gz) if you want to reproduce these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pitfalls in data mungling with external reference panel\n",
    "\n",
    "1. Genomic coordinate can be zero- or one-based. That means SNP positions can start from **start from 0**, instead of 1. This is indeed the case for 1000 Genomes data. In order to be consistent with one-based summary statistics, we need to **add 1** to all 1000 genome SNPs position.\n",
    "2. Reference and alternative alleles may mismatch between summary statistics and the reference panel. If it is a simple ref / alt flip we need to also flip the coding of genotypes or sign of summary statistics to adjust for the effect size direction. If it is strand flip we need to convert summary statistics and genotype data to use the same strand first and take from there. \n",
    "3. When strand flip is involved, cases such as A/T and C/G are no longer identifiable -- whether it be strand flip or ref / alt flip. We will have to remove these locus (having A/T or C/G genotypes) from analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Data extraction steps\n",
    "\n",
    "1. Denote summary statistics matrix in the specific loci as $S_1$, and annotation (prior) matrix as $A_1$. The number of row in these two matrices is the number of SNPs in summary statistics of the study of interest.\n",
    "2. Extract corresponding genotype from reference panel in VCF format for this loci. Denote this genotype matrix as $G_1$. Rows are SNPs, columns are population genotypes. \n",
    "    - Genotype coding: we use numeric coding 0, 1 and 2 indicating the number of \"non-reference allele\". In other words we have the following \"mapping rule\":\n",
    "        ```\n",
    "        0|0  ->  0\n",
    "        1|0  ->  1\n",
    "        1|1  ->  2\n",
    "        2|0  ->  1\n",
    "        2|1  ->  2\n",
    "        2|2  ->  2\n",
    "        ```\n",
    "    - Non-variant sites (lines having identical genotypes for everybody) will be removed.\n",
    "    \n",
    "3. Find overlapped SNPs of $S_1$ and $G_1$, then extract new genotype matrix from $G_1$ excluding non-overlaps, denote as $G_2$, and new summary statistics matrix from $S_1$ excluding non-overlaps, denote as $S_2$.\n",
    "4. Compare $G_2$ and $S_2$'s reference and alternative allele, flip coding as necessary, generating new summay statistic matrix $S_3$ and new genotype matrix $G_3$. There could be several situations as follows:\n",
    "\n",
    "    - completely identical;\n",
    "    - Not identical, but identical after switching ref and alt in $S_2$: add opposite sign for z score and beta; does not apply to `A/T`, `T/A`, `G/C` or `C/G` for `ref/alt`;\n",
    "    - Not identical, but identical after strand flip ref and alt in $S_2$: keep the sign of z score and beta; does not apply to `A/T`, `T/A`, `G/C` or `C/G` for `ref/alt`;\n",
    "    - Not identical, but identical after strand flip ref and alt then swith their positions: add opposite sign for z score and beta; only apply to `A/G`, `G/A`, `A/C`, `C/A`, `T/C`, `C/T`, `T/G` and `G/T`.\n",
    "    - Not identical, `A/T`, `T/A`, `G/C` or `C/G` for `ref/alt`: consider this situation at last; if flip strand has applied in this LD block, then flip strand and keep the sign of z score and beta; if not, switch ref and alt of $S_2$, add opposite sign for z score and beta.\n",
    "    - Not identical after previous 5 substeps: drop.\n",
    "5. Calculate row correlation matrix of $G_3$, denote as $R$. The number of rows and columns of $R$ is the number of SNPs in $G_3$.\n",
    "6. Obtain overlapped SNPs for $S_3$ and $A_1$ and use overlapped SNPs to generate new annotation/prior $A_2$.\n",
    "\n",
    "Notice that if genotype data used to generate the summary statistics is available as reference panel, there should not be a need for 4. But it is a good double-check anyways to do 4 -- we therefore provide an option to specify whether or not we expect step 4 is unnecessary; and if so, throw an error when we found discrepency in 4, instead of trying to fix those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Outputs\n",
    "- $S_3$: the number of rows of $S_3$ is the same with $A_2$. It can be 4 columns, `chr:pos  beta  se  ID`, or 3 columns, `chr:pos z ID`.\n",
    "- $R$: correlation matrix, #SNPs $\\times$ #SNPs of $G_3$.\n",
    "- $A_2$: adjusted annotation/prior information, the number of rows is the same with $S_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Software requirement\n",
    "\n",
    "- Python package `cyvcf2`, `conda install -c bioconda cyvcf2`\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gaow/Documents/GIT/github/fine-mapping"
     ]
    }
   ],
   "source": [
    "%cd ~/GIT/github/fine-mapping\n",
    "this_nb = 'workflow/summary_statistics_wrangler.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> ! sos run workflow/summary_statistics_wrangler.ipynb -h<br></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run workflow/summary_statistics_wrangler.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  prepare_1KG_reference\n",
      "  default\n",
      "\n",
      "Global Workflow Options:\n",
      "  --reference . (as path)\n",
      "                        Reference panel, either VCF file or a manifest file each\n",
      "                        line is \"chrom number <white space> VCF file name\" the\n",
      "                        manifest file have to be in the same folder as the VCF\n",
      "                        files it refers to.\n",
      "  --loci . (as path)\n",
      "                        Loci file\n",
      "  --ss-data . (as path)\n",
      "                        summary statistics\n",
      "  --[no-]strand-flip (default to True)\n",
      "                        Use --no-strand-flip to set it to false if you are sure\n",
      "                        there is no strand_flip involved\n",
      "  --[no-]ref-flip (default to True)\n",
      "                        Use --no-ref-flip to set it to false if you are sure\n",
      "                        there is no reference / alternative mismatch involved\n",
      "\n",
      "Sections\n",
      "  prepare_1KG_reference: Get information about a specified race\n",
      "    Workflow Options:\n",
      "      --race EUR\n",
      "                        Race identifier in 1000 genomes\n",
      "      --panel-meta . (as path)\n",
      "                        Path to\n",
      "                        `integrated_call_samples_v3.20130502.ALL.panel.txt`\n",
      "  default_1:            Get genotype matrix\n",
      "    Workflow Options:\n",
      "      --adjust-panel-position 0 (as int)\n",
      "                        when set to N (typically 0, 1 or -1) the genomic\n",
      "                        position in the panel will be adjusted by N, ie.\n",
      "                        position = position + N This is useful when summary\n",
      "                        stats and reference panel have different coordinates\n",
      "                        (off by 1 position)\n",
      "  default_2:\n",
      "    Workflow Options:\n",
      "      --columns 1 2 3 4 5 6 (as list)\n",
      "                        Have to be 5 or 6 numbers indicating the required\n",
      "                        columns to be extracted from summary statistics file 6\n",
      "                        numbers for ['chr','bp','ref','alt','beta','se'] 5\n",
      "                        numbers fo ['chr','bp','ref','alt','z']\n",
      "      --loci-column 0 (as int)\n",
      "                        ID column index, optional\n",
      "      --[no-]header (default to True)\n",
      "                        use --no-header to indicate that input summary\n",
      "                        statistics file does not have a header\n",
      "  default_3:\n",
      "    Workflow Options:\n",
      "      --annotation 'name:/path/to/annotation/file'\n"
     ]
    }
   ],
   "source": [
    "! sos run {this_nb} -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Reference panel, either VCF file or a manifest file each line is \"chrom number <white space> VCF file name\"\n",
    "# the manifest file have to be in the same folder as the VCF files it refers to.\n",
    "parameter: reference = path()\n",
    "# Loci file\n",
    "parameter: loci = path()\n",
    "# summary statistics\n",
    "parameter: ss_data = path()\n",
    "# Use --no-strand-flip to set it to false\n",
    "# if you are sure there is no strand_flip involved\n",
    "parameter: strand_flip = True\n",
    "# Use --no-ref-flip to set it to false\n",
    "# if you are sure there is no reference / alternative mismatch involved\n",
    "parameter: ref_flip = True\n",
    "\n",
    "# Decide whether or not reference panel matches summary statistics\n",
    "if not ref_flip and not strand_flip:\n",
    "    strictly_match = True\n",
    "else:\n",
    "    strictly_match = False\n",
    "\n",
    "# Check if files exist\n",
    "fail_if(not reference.is_file(), msg = 'Please specify valid path for --reference')\n",
    "fail_if(not loci.is_file(), msg = 'Please specify valid path for --loci')\n",
    "fail_if(not ss_data.is_file(), msg = 'Please specify valid path for --ss-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Preparing external reference genotypes\n",
    "\n",
    "For 1000 genomes data, using `integrated_call_samples_v3.20130502.ALL.panel.txt` file to extract sample ID for given population (eg `EUR`) and subset the data. We only need to run it once.\n",
    "\n",
    "If loci manifest is provided it generate both a new loci manifest file and the files it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run workflow/summary_statistics_wrangler.ipynb prepare_1KG_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get information about a specified race\n",
    "[prepare_1KG_reference]\n",
    "depends: executable(\"bcftools\"), executable(\"tabix\")\n",
    "# Race identifier in 1000 genomes\n",
    "parameter: race = \"EUR\"\n",
    "# Path to `integrated_call_samples_v3.20130502.ALL.panel.txt`\n",
    "parameter: panel_meta = path()\n",
    "stop_if(not panel_meta.is_file(), msg = 'Please specify valid path for --panel-meta')\n",
    "\n",
    "if str(reference).endswith('vcf.gz'):\n",
    "    chroms = ['0']\n",
    "    genotypes = [reference]\n",
    "else:\n",
    "    manifest = [x.strip().split() for x in open(f'{reference:a}').readlines()]\n",
    "    meta = [x[0] for x in manifest]\n",
    "    genotypes = [x[1] for x in manifest]\n",
    "\n",
    "input: genotypes, group_by = 1, paired_with = dict(chrom=chroms)\n",
    "output: f\"{_input:nn}.{race}.vcf.gz\"\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '5m', mem = '2G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "\n",
    "bash: expand = True\n",
    "    grep -w \"{race}\" {panel_meta} | cut -f 1 > {_output:nn}_extracted.txt\n",
    "    bcftools view -S {_output:nn}_extracted.txt {_input} -Oz > {_output}\n",
    "    tabix -p vcf {_output}\n",
    "    rm {_output:nn}_extracted.txt\n",
    "\n",
    "if not str(reference).endswith('vcf.gz'):\n",
    "    with open(f\"{reference:n}.{race}.{reference:x}\", 'w') as f:\n",
    "        for item in _output:\n",
    "            f.write(f'{item.chrom} {item}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Obtain reference genotypes for given loci: matrix `𝐺1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get genotype matrix\n",
    "[default_1 (get genotype)]\n",
    "# when set to N (typically 0, 1 or -1) the genomic position\n",
    "# in the panel will be adjusted by N, ie. position = position + N\n",
    "# This is useful when summary stats and reference panel have different coordinates (off by 1 position)\n",
    "parameter: adjust_panel_position = 0\n",
    "depends: Py_Module('cyvcf2')\n",
    "\n",
    "chunks = [x.strip().split() for x in open(f'{loci:a}').readlines() if not x.strip().startswith('#')]\n",
    "\n",
    "if str(reference).endswith('vcf.gz'):\n",
    "    genotype = reference\n",
    "else:\n",
    "    genotype = dict([tuple(x.strip().split()) for x in open(f'{reference:a}').readlines()])\n",
    "    for k in genotype:\n",
    "        genotype[k] = os.path.join(f'{reference:ad}', genotype[k])\n",
    "\n",
    "input: group_by = 1, for_each = 'chunks'\n",
    "output: f\"{ss_data:n}/{_chunks[-1] if len(_chunks) > 3 else '%s_%s_%s' % (_chunks[0], _chunks[1], _chunks[2])}.pkl\", group_with = 'chunks'\n",
    "task: trunk_workers = 1, trunk_size = 80, walltime = '5m', mem = '5G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "\n",
    "python3: expand = \"${ }\"\n",
    "    from cyvcf2 import VCF\n",
    "    import pandas as pd\n",
    "    chromosome = '${_chunks[0].replace(\"chr\",\"\")}'\n",
    "    panel = ${path(genotype[_chunks[0].replace(\"chr\",\"\")] if isinstance(genotype, dict) else genotype):ar}\n",
    "    # loci region\n",
    "    queryid =  chromosome + \":\" + \"${_chunks[1]}\" + \"-\" + \"${_chunks[2]}\"\n",
    "    off_set = ${adjust_panel_position}\n",
    "    # scan VCF chunk\n",
    "    vcf = VCF(panel, gts012=False)\n",
    "    res = []\n",
    "    ## Attention: 1000 Genome position start from 0, not 1! So need to +1 for variant.start\n",
    "    for variant in vcf(queryid):\n",
    "        for i in range(len(variant.ALT)):\n",
    "            line = [f'{variant.CHROM}:{variant.start+off_set}', \n",
    "                    variant.REF, variant.ALT[i]] + \\\n",
    "                    [x[:-1].count(i+1) for x in variant.genotypes]\n",
    "            if len(set(line[3:])) == 1:\n",
    "                # remove non-variant site in reference panel\n",
    "                continue\n",
    "            res.append(line)\n",
    "    gt = pd.DataFrame(res, columns = ['ID', 'ref', 'alt'] + vcf.samples)\n",
    "    gt.to_pickle(${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Obtain summary statistics and the matching genotype correlation for given loci\n",
    "\n",
    "Obtain genotype matrix 𝐺2, summary statistics 𝑆2, compare ref/alt in 𝑆2 and 𝐺2 => 𝑆3 and 𝐺3, calculate 𝑅=row_corr(𝐺3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_2 (get summary stats)]\n",
    "# Have to be 5 or 6 numbers indicating the required columns\n",
    "# to be extracted from summary statistics file\n",
    "# 6 numbers for ['chr','bp','ref','alt','beta','se']\n",
    "# 5 numbers fo ['chr','bp','ref','alt','z']\n",
    "parameter: columns = [1,2,3,4,5,6]\n",
    "# ID column index, optional\n",
    "parameter: loci_column = 0\n",
    "# use --no-header to indicate that input summary statistics file does not have a header\n",
    "parameter: header = True\n",
    "fail_if(len(columns) not in (5,6), msg = 'Input column ID has to be of length 5 or 6.')\n",
    "output: summary_stats = f\"{_input:n}/{_input:bn}.summary_stats.txt\", ld_matrix = f\"{_input:n}/{_input:bn}.LD.txt\"\n",
    "task: trunk_workers = 1, trunk_size = 80, walltime = '10m', mem = '5G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "\n",
    "python3: expand=\"${ }\", stdout = f'{_output[0]:n}.allele_flip.log'\n",
    "    import pandas as pd, numpy as np\n",
    "    import datetime, sys\n",
    "\n",
    "    def exit_if_empty(x):\n",
    "        if x.empty:\n",
    "            open(\"${_output['summary_stats']}\", 'w').close()\n",
    "            open(\"${_output['ld_matrix']}\", 'w').close()\n",
    "            sys.exit(0)\n",
    "\n",
    "    print(datetime.datetime.now(),\"\\n\")\n",
    "    locus = ${_input._chunks}\n",
    "    col_to_use = ${[x-1 for x in columns]}\n",
    "    loci_col = ${loci_column}\n",
    "    if loci_col > 0:\n",
    "        col_to_use.append(loci_col-1)\n",
    "    col_ranks = [x for x in sorted(range(len(col_to_use)), key=col_to_use.__getitem__)]\n",
    "    S1 = pd.read_table(${ss_data:ar}, compression='gzip', header = ${0 if header else None}, index_col=False, usecols = col_to_use)\n",
    "    if loci_col > 0:\n",
    "        columns = ['chr','bp','ref','alt','beta','se','locus_id'] if S1.shape[1] == 7 else ['chr','bp','ref','alt','z','locus_id']\n",
    "    else:\n",
    "        columns = ['chr','bp','ref','alt','beta','se'] if S1.shape[1] == 6 else ['chr','bp','ref','alt','z']\n",
    "    S1.columns = [columns[r] for r in col_ranks]\n",
    "    S1 = S1[columns]\n",
    "    if \"locus_id\" in S1.columns and len(locus) == 4:\n",
    "        S1 = S1[(S1[\"chr\"] == locus[0]) & (S1[\"bp\"] >= int(locus[1])) & (S1[\"bp\"] < int(locus[2])) & (S1[\"locus_id\"] == locus[3])]\n",
    "    else:\n",
    "        S1 = S1[(S1[\"chr\"] == locus[0]) & (S1[\"bp\"] >= int(locus[1])) & (S1[\"bp\"] < int(locus[2]))]\n",
    "    exit_if_empty(S1)\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"S1\",len(S1)))\n",
    "    S1[\"chr\"] = S1[\"chr\"].str.replace(\"chr\", \"\")\n",
    "    S1[\"ID\"] = S1[\"chr\"] + \":\" + S1[\"bp\"].map(str)\n",
    "    G1 = pd.read_pickle(\"${_input}\")\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"G1\",len(G1)))\n",
    "    # S2\n",
    "    S2 = S1[S1[\"ID\"].isin(G1[\"ID\"])]\n",
    "    exit_if_empty(S2)\n",
    "    #S2[\"ID\"] = S2[\"ID\"].fillna\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"S2\",len(S2)))\n",
    "    # 𝐺2\n",
    "    G2 = G1[G1[\"ID\"].isin(S1[\"ID\"])].copy()\n",
    "    exit_if_empty(G2)\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"G2\",len(G2)))\n",
    "    # 𝑆2 and 𝐺2 : reference and alternative\n",
    "    # at ta cg gc\n",
    "    def gt(s1,s2,s3,s4):\n",
    "        if (s1+s2 == \"AT\" and s3+s4 == \"TA\") or (s1+s2 == \"GC\" and s3+s4 == \"CG\" ) or (s1+s2 == \"TA\" and s3+s4 == \"AT\") or (s1+s2 == \"CG\" and s3+s4 == \"GC\"):\n",
    "            return ${0 if strand_flip else 1}\n",
    "        else:\n",
    "            return 1\n",
    "    # flip\n",
    "    def atcg(inp):\n",
    "        if inp == \"A\":\n",
    "            return \"T\"\n",
    "        elif inp == \"T\":\n",
    "            return \"A\"\n",
    "        elif inp == \"G\":\n",
    "            return \"C\"\n",
    "        elif inp == \"C\":\n",
    "            return \"G\"\n",
    "    # S3\n",
    "    FLIPD = []\n",
    "    # adjusted\n",
    "    ADJ = 0\n",
    "    # equal\n",
    "    EQUAL = 0\n",
    "    # flipped by strand \n",
    "    FLIPNUM = 0\n",
    "    # flipped by reference and alternative\n",
    "    REFALTNUM = 0\n",
    "    \n",
    "    # keep at/ta/cg/gc line numbers\n",
    "    at_cg_id = []\n",
    "    for i in range(len(S2)):\n",
    "        old_id = S2.iloc[i,0] + \":\" + str(S2.iloc[i,1]) + \":\" + S2.iloc[i,2] + \":\" + S2.iloc[i,3]\n",
    "        line = S2.iloc[i,:-1].tolist() + [old_id]\n",
    "        if set([S2.iloc[i,2],S2.iloc[i,3]]) != set([G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0]]):\n",
    "            # print (S2.iloc[i,1],S2.iloc[i,2],S2.iloc[i,3],S2.iloc[i,-1], G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0])\n",
    "            ADJ += 1\n",
    "        # not at/ta/gc/cg\n",
    "        if gt(S2.iloc[i,2],S2.iloc[i,3],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0])==1:\n",
    "            if G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0] == S2.iloc[i,2] and G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0] == S2.iloc[i,3]:\n",
    "                EQUAL+=1\n",
    "                FLIPD.append(line)\n",
    "            elif G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0] == S2.iloc[i,2] and G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0] == S2.iloc[i,3]:\n",
    "                REFALTNUM+=1\n",
    "                line[2], line[3] = line[3], line[2]\n",
    "                line[4] = -line[4]\n",
    "                FLIPD.append(line)\n",
    "            elif atcg(S2.iloc[i,2]) == G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0] and atcg(S2.iloc[i,3]) == G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0]:\n",
    "                FLIPNUM+=1\n",
    "                line[2], line[3] = atcg(line[2]), atcg(line[3])\n",
    "                FLIPD.append(line)\n",
    "            elif atcg(S2.iloc[i,2]) == G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0] and atcg(S2.iloc[i,3]) == G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0]:\n",
    "                REFALTNUM+=1\n",
    "                FLIPNUM+=1\n",
    "                line[2], line[3] = atcg(line[3]), atcg(line[2])\n",
    "                line[4] = -line[4]\n",
    "                FLIPD.append(line)\n",
    "        # at/ta/cg/gc only do ref/alt flip\n",
    "        elif gt(S2.iloc[i,2],S2.iloc[i,3],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0],G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0])==0:\n",
    "            if G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0] == S2.iloc[i,2] and G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0] == S2.iloc[i,3]:\n",
    "                EQUAL+=1\n",
    "                FLIPD.append(line)\n",
    "                at_cg_id.append(len(FLIPD) - 1)\n",
    "            elif G2[G2[\"ID\"]==S2.iloc[i,-1]][\"alt\"].iloc[0] == S2.iloc[i,2] and G2[G2[\"ID\"]==S2.iloc[i,-1]][\"ref\"].iloc[0] == S2.iloc[i,3]:\n",
    "                REFALTNUM+=1\n",
    "                line[2], line[3] = line[3], line[2]\n",
    "                line[4] = -line[4]                \n",
    "                FLIPD.append(line)\n",
    "                at_cg_id.append(len(FLIPD) - 1)\n",
    "    if ${strictly_match} and (FLIPNUM > 0 or REFALTNUM > 0):\n",
    "        raise ValueError(f\"Strict panel match failed for locus {locus}.\")\n",
    "    #\n",
    "    MISMATCH = len(S2) - len(FLIPD)\n",
    "    if FLIPNUM > 0:\n",
    "        # flips involved, we need to remove at/ta/cg/gc\n",
    "        FLIPD = [i for j, i in enumerate(FLIPD) if j not in at_cg_id]\n",
    "    else:\n",
    "        at_cg_id = []\n",
    "    columns = [x for x in S1.columns.tolist() if x != 'ID'] + [\"original_ID\"]\n",
    "    S3 = pd.DataFrame(FLIPD, columns=columns)\n",
    "    S3[\"ID\"] = S3[\"chr\"] +\":\"+S3[\"bp\"].map(str) + \":\" + S3[\"ref\"] + \":\" + S3[\"alt\"]\n",
    "    G2[\"ID\"] = G2[['ID', 'ref', 'alt']].apply(lambda x: ':'.join(x), axis=1)\n",
    "    # 𝐺3\n",
    "    G3 = G2[G2[\"ID\"].isin(S3[\"ID\"])]\n",
    "    assert G3.shape[0] == S3.shape[0]\n",
    "    exit_if_empty(S3)\n",
    "    # recover original ID\n",
    "    S3[\"ID\"] = S3.pop(\"original_ID\")\n",
    "    S3 = S3.sort_values(by = [\"chr\", \"bp\"]).drop(columns = [x for x in S3.columns if not x in ('ID', 'z', 'beta', 'se')])\n",
    "    columns = S3.columns.tolist()\n",
    "    columns.insert(0, columns.pop(columns.index('ID')))\n",
    "    S3[columns].to_csv(\"${_output['summary_stats']}\",sep=\"\\t\",index=False,header=None)\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"S3\",len(S3)))\n",
    "    print(\"equal:%s\\nflipped by strand:%d\\nflipped by reference and alternative:%d\\ntotal adjusted:%d\\ntotal mismatch:%d\\ntotal A/T and C/G removed:%d\" % (EQUAL,FLIPNUM,REFALTNUM,ADJ,MISMATCH,len(at_cg_id)))\n",
    "    # 𝑅=row_corr(𝐺3) OUT \\\n",
    "    np.savetxt(\"${_output['ld_matrix']}\", np.corrcoef(G3.drop(columns=['ID', \"ref\",\"alt\"]).values) if G3.shape[0] > 1 else np.array([[1]]), fmt = '%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Obtain annotation $A_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_3 (get annotations)]\n",
    "parameter: annotation = \"name:/path/to/annotation/file\"\n",
    "anno, prior_file = annotation.split(':')\n",
    "prior_file = path(prior_file)\n",
    "stop_if(not prior_file.is_file(), msg = 'Quit because annotation file is not found')\n",
    "input: [x for x in output_from(-1) if os.stat(x).st_size > 0], group_by = 2\n",
    "output: f\"{_input[0]:nn}.{anno}.txt\"\n",
    "task: trunk_workers = 1, trunk_size = 80, walltime = '10m', mem = '2G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "\n",
    "python3: expand=\"${ }\"\n",
    "    import pandas as pd\n",
    "    A1 = pd.read_table(\"${prior_file}\", compression='gzip', sep=\"\\s+\", header=None)\n",
    "    A1.columns = [\"ID\",\"VALUE\"]\n",
    "    S3 = pd.read_table(\"${_input[0]}\",sep=\"\\t\", header=None)\n",
    "    A2 = A1[A1[\"ID\"].isin(S3.iloc[:,0])][[\"ID\",\"VALUE\"]]\n",
    "    A2.to_csv(\"${_output}\",sep=\"\\t\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Run preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "eQTL data example run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run workflow/summary_statistics_wrangler.ipynb --reference /home/gaow/tmp/19-Dec-2018/1KG_EUR/test.manifest \\\n",
    "        --loci /home/gaow/tmp/19-Dec-2018/metasoft/one_loci.txt \\\n",
    "        --ss-data /home/gaow/tmp/19-Dec-2018/metasoft/gtex_metasoft_summary_stats.gz \\\n",
    "        --columns 1 2 3 4 6 7 --loci-column 5 --no-header \\\n",
    "        -q none -j 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "GWAS data example run (with annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run workflow/summary_statistics_wrangler.ipynb --reference /home/gaow/tmp/19-Dec-2018/1KG_EUR/test.manifest \\\n",
    "        --loci /home/gaow/tmp/19-Dec-2018/SCZ/chunks.list \\\n",
    "        --ss-data /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics.gz \\\n",
    "        --columns 1 2 3 4 6 7 --adjust-panel-position 1 \\\n",
    "        --annotation atac-seq:/home/gaow/tmp/19-Dec-2018/SCZ/Annotation_atac-seq.gz \\\n",
    "        -q none -j 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**For UChicago midway users**: to run these pipelines on RCC cluster, please take a look at `midway2.yml` file (found in this repository), modify it as you see fit, and replace `-q none -j 8` with: \n",
    "\n",
    "```\n",
    "-c workflow/midway2.yml -q midway2 -J 40\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Results preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Reference panel matching summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.allele_flip.log</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.allele_flip.log (271 B):</div>"
      ],
      "text/plain": [
       "\n",
       "> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.allele_flip.log (271 B):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">13 lines</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-21 12:48:22.194216 \n",
      "\n",
      "SAMPLE: S1\tLENGTH:184\n",
      "SAMPLE: G1\tLENGTH:1034\n",
      "SAMPLE: S2\tLENGTH:182\n",
      "SAMPLE: G2\tLENGTH:182\n",
      "SAMPLE: S3\tLENGTH:182\n",
      "equal:94\n",
      "flipped by strand:0\n",
      "flipped by reference and alternative:88\n",
      "total adjusted:0\n",
      "total mismatch:0\n",
      "total A/T and C/G removed:0"
     ]
    }
   ],
   "source": [
    "%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641/chr9_84630941_84813641.allele_flip.log -n --limit 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Summary statistics matrix S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.summary_stats.txt</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.summary_stats.txt (6.0 KiB):</div>"
      ],
      "text/plain": [
       "\n",
       "> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.summary_stats.txt (6.0 KiB):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">182 lines (10 displayed, see --limit)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:84631606:A:G\t-0.024303\t0.0176\n",
      "9:84632950:A:G\t0.021595\t0.0174\n",
      "9:84635599:A:G\t-0.0587\t0.0555\n",
      "9:84636858:A:T\t0.011901\t0.025\n",
      "9:84636881:A:G\t0.022104\t0.0174\n",
      "9:84638794:T:C\t-0.038596\t0.0337\n",
      "9:84638880:A:C\t-0.121004\t0.0643\n",
      "9:84639829:A:G\t-0.058703\t0.0558\n",
      "9:84640522:A:G\t0.024605000000000002\t0.0175\n",
      "9:84640717:T:C\t-0.046501999999999995\t0.0345"
     ]
    }
   ],
   "source": [
    "%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641/chr9_84630941_84813641.summary_stats.txt -n -l 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "### Annotation/prior matrix A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.Annotation_atac-seq.txt</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.Annotation_atac-seq.txt (4.6 KiB):</div>"
      ],
      "text/plain": [
       "\n",
       "> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.Annotation_atac-seq.txt (4.6 KiB):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">182 lines (10 displayed, see --limit)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:84631606:A:G\t7.3588e-05\n",
      "9:84632950:A:G\t7.3588e-05\n",
      "9:84635599:A:G\t7.3588e-05\n",
      "9:84636858:A:T\t7.3588e-05\n",
      "9:84636881:A:G\t7.3588e-05\n",
      "9:84638794:T:C\t7.3588e-05\n",
      "9:84638880:A:C\t7.3588e-05\n",
      "9:84639829:A:G\t0.00023664\n",
      "9:84640522:A:G\t7.3588e-05\n",
      "9:84640717:T:C\t7.3588e-05"
     ]
    }
   ],
   "source": [
    "%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641/chr9_84630941_84813641.Annotation_atac-seq.txt -n -l 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### 𝑅: LD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.16511</td>\n",
       "      <td>-0.08450</td>\n",
       "      <td>-0.05728</td>\n",
       "      <td>-0.16511</td>\n",
       "      <td>-0.09862</td>\n",
       "      <td>0.26204</td>\n",
       "      <td>-0.08450</td>\n",
       "      <td>-0.16958</td>\n",
       "      <td>-0.09553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19386</td>\n",
       "      <td>0.23134</td>\n",
       "      <td>0.28432</td>\n",
       "      <td>-0.09078</td>\n",
       "      <td>-0.13401</td>\n",
       "      <td>-0.06267</td>\n",
       "      <td>0.17942</td>\n",
       "      <td>-0.13495</td>\n",
       "      <td>0.22525</td>\n",
       "      <td>0.36580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.16511</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>-0.12045</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.08884</td>\n",
       "      <td>-0.05748</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>0.99163</td>\n",
       "      <td>-0.08591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08443</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>-0.02916</td>\n",
       "      <td>0.70175</td>\n",
       "      <td>-0.08891</td>\n",
       "      <td>0.29019</td>\n",
       "      <td>0.00745</td>\n",
       "      <td>-0.07106</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.04784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.08450</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.06756</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>-0.05645</td>\n",
       "      <td>-0.02924</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.35389</td>\n",
       "      <td>-0.05572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11853</td>\n",
       "      <td>0.27991</td>\n",
       "      <td>-0.02222</td>\n",
       "      <td>-0.02840</td>\n",
       "      <td>-0.04367</td>\n",
       "      <td>0.04285</td>\n",
       "      <td>0.29319</td>\n",
       "      <td>0.00498</td>\n",
       "      <td>-0.04503</td>\n",
       "      <td>-0.04552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.05728</td>\n",
       "      <td>-0.12045</td>\n",
       "      <td>-0.06756</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12045</td>\n",
       "      <td>-0.00022</td>\n",
       "      <td>-0.05648</td>\n",
       "      <td>-0.06756</td>\n",
       "      <td>-0.11798</td>\n",
       "      <td>-0.01960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.19109</td>\n",
       "      <td>-0.01319</td>\n",
       "      <td>0.00848</td>\n",
       "      <td>-0.08558</td>\n",
       "      <td>-0.08434</td>\n",
       "      <td>-0.01702</td>\n",
       "      <td>-0.01805</td>\n",
       "      <td>-0.06870</td>\n",
       "      <td>0.00707</td>\n",
       "      <td>-0.08792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.16511</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>-0.12045</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.08884</td>\n",
       "      <td>-0.05748</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>0.99163</td>\n",
       "      <td>-0.08591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08443</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>-0.02916</td>\n",
       "      <td>0.70175</td>\n",
       "      <td>-0.08891</td>\n",
       "      <td>0.29019</td>\n",
       "      <td>0.00745</td>\n",
       "      <td>-0.07106</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.04784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1        2        3        4        5        6        7    \\\n",
       "0  1.00000 -0.16511 -0.08450 -0.05728 -0.16511 -0.09862  0.26204 -0.08450   \n",
       "1 -0.16511  1.00000  0.35094 -0.12045  1.00000 -0.08884 -0.05748  0.35094   \n",
       "2 -0.08450  0.35094  1.00000 -0.06756  0.35094 -0.05645 -0.02924  1.00000   \n",
       "3 -0.05728 -0.12045 -0.06756  1.00000 -0.12045 -0.00022 -0.05648 -0.06756   \n",
       "4 -0.16511  1.00000  0.35094 -0.12045  1.00000 -0.08884 -0.05748  0.35094   \n",
       "\n",
       "       8        9     ...         172      173      174      175      176  \\\n",
       "0 -0.16958 -0.09553   ...     0.19386  0.23134  0.28432 -0.09078 -0.13401   \n",
       "1  0.99163 -0.08591   ...    -0.08443  0.00125 -0.02916  0.70175 -0.08891   \n",
       "2  0.35389 -0.05572   ...    -0.11853  0.27991 -0.02222 -0.02840 -0.04367   \n",
       "3 -0.11798 -0.01960   ...    -0.19109 -0.01319  0.00848 -0.08558 -0.08434   \n",
       "4  0.99163 -0.08591   ...    -0.08443  0.00125 -0.02916  0.70175 -0.08891   \n",
       "\n",
       "       177      178      179      180      181  \n",
       "0 -0.06267  0.17942 -0.13495  0.22525  0.36580  \n",
       "1  0.29019  0.00745 -0.07106 -0.12696 -0.04784  \n",
       "2  0.04285  0.29319  0.00498 -0.04503 -0.04552  \n",
       "3 -0.01702 -0.01805 -0.06870  0.00707 -0.08792  \n",
       "4  0.29019  0.00745 -0.07106 -0.12696 -0.04784  \n",
       "\n",
       "[5 rows x 182 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_table('/home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641/chr9_84630941_84813641.LD.txt', sep = ' ', header = None, nrows = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A"
    ],
    [
     "R",
     "ir",
     "R",
     "#DCDCDA"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.17.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
